<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Hello CP</title>
    <link href="/article/10116.html"/>
    <url>/article/10116.html</url>
    
    <content type="html"><![CDATA[<h1 id="你好，CP"><a href="#你好，CP" class="headerlink" title="你好，CP"></a>你好，CP</h1><p>这是我继<a href="https://www.gentlecp.com">求索博客</a>后，重新打造的一个博客，用于分享我个人的学习经验和生活感悟，原博客大概率还会保留（同步这里的内容），但主要创作的内容会向本博客迁移，原因主要有几点：  </p><ol><li>原博客基于<code>wordpress</code>,虽然功能很强大，但同时也造成了博客的臃肿，加载的时候缓慢，现有博客基于<code>hexo</code>，静态资源加载更快</li><li>原博客的文章内容直接保存在服务器上，虽然也能导出，但数据不是原始的md文件，这不利于我的迁移工作，现博客的数据都直接存储在<code>github</code>，安全又方便</li><li>除去花里胡哨的，做核心的内容。正如博客标语中写的，<code>Simple is the best!</code>，在本博客中，将致力于将文章内容创作地更好，将其他细枝末节的东西忽略</li><li>一次全新的尝试</li></ol><p>新的学期，希望能够写出更多优质的文章，提升自己，帮助更多人。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>py2neo 快速上手</title>
    <link href="/article/53311.html"/>
    <url>/article/53311.html</url>
    
    <content type="html"><![CDATA[<h1 id="py2neo-快速上手"><a href="#py2neo-快速上手" class="headerlink" title="py2neo 快速上手"></a>py2neo 快速上手</h1><h2 id="py2neo简介"><a href="#py2neo简介" class="headerlink" title="py2neo简介"></a>py2neo简介</h2><p><code>neo4j</code> 是目前最流行的图数据库，在建立知识图谱的时候经常用于数据的存储和检索，<code>neo4j</code> 相较<code>mysql</code> 等其他关系型数据库最大的特点就是非常容易查看数据与数据之间的联系，它将所有数据转换成<strong>图</strong>的形式，让使用者能够快速发现蕴含的联系。下面是<code>neo4j</code> 的主要界面和相应的数据图例（射雕英雄传人物知识图谱）。</p><p><img src="https://gitee.com/gentlecp/ImgUrl/raw/master/20210219123737.png" alt="image-20210219123737557"></p><p>但是，直接地对<code>neo4j</code>操作，虽然有相应的<code>cypher</code>语句，却并不适合在编程的过程中操作。<code>py2neo</code>就是一个连接了<code>python</code>到<code>neo4j</code>数据库的库，它不仅让程序本身可以直接执行<code>cypher</code>语句创建图数据库，也提供了更人性化的操作，符合面向对象编程的思想。</p><h2 id="neo4j"><a href="#neo4j" class="headerlink" title="neo4j"></a>neo4j</h2><p>在进入使用<code>py2neo</code>之前，还是要先安装好<code>neo4j</code>，了解一些基础的<code>cypher</code>语句用法（虽然我自己基本不怎么用它），然而<code>neo4j</code>的介绍不是本文的重点，网上已有大篇幅的文章去讲述怎么使用它。因此，这里我会给出必要的操作步骤（避免再去看冗长的文档）和我个人推荐的博客文章（包含更为详细的说明）。</p><h3 id="安装与启动"><a href="#安装与启动" class="headerlink" title="安装与启动"></a>安装与启动</h3><ul><li>安装</li></ul><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">wget https:&#x2F;&#x2F;neo4j.com&#x2F;artifact.php?name&#x3D;neo4j-community-3.4.1-unix.tar.gztar -zxvf neo4j-community-3.4.1.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></div><blockquote><p>安装的环境建议放在linux系统下，这是我个人的爱好也是一个建议（所有部署的服务、数据库等东西都放到linux系统中），不仅仅是安装更加方便，同时也可以避免后续许多的不便，如果你主力系统是windows，那么虚拟机+网络桥接模式是一个不错的选择</p></blockquote><ul><li>启动</li></ul><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python">cd <span class="token builtin">bin</span><span class="token operator">/</span> <span class="token punctuation">.</span><span class="token operator">/</span>neo4j start   <span class="token comment"># console | start | stop | restart | status</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></div><p>当发现启动成功后，进入<code>http://localhost:7474</code></p><ul><li>推荐阅读文献<ul><li><a href="https://juejin.cn/post/6844904180906197006">https://juejin.cn/post/6844904180906197006</a></li><li><a href="https://zhuanlan.zhihu.com/p/88745411">https://zhuanlan.zhihu.com/p/88745411</a></li></ul></li></ul><h2 id="py2neo使用"><a href="#py2neo使用" class="headerlink" title="py2neo使用"></a>py2neo使用</h2><blockquote><p>py2neo支持直接执行<code>cypher</code>语句，如果你对<code>cypher</code>语句熟悉的话，将你要执行的操作转换成<code>cypher</code>语句，再让<code>py2neo</code>执行或许是不错的选择，否则，我更推荐你使用<code>py2neo</code>提供的接口</p></blockquote><ul><li><a href="https://py2neo.org/v4/">官方文档</a></li></ul><h3 id="连接数据库"><a href="#连接数据库" class="headerlink" title="连接数据库"></a>连接数据库</h3><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> py2neo <span class="token keyword">import</span> <span class="token operator">*</span>NEO4J_URL <span class="token operator">=</span> <span class="token string">'http://localhost:7474'</span>NEO4J_USERNAME <span class="token operator">=</span> <span class="token string">'neo4j'</span>NEO4J_PASSWORD <span class="token operator">=</span> <span class="token string">'123456'</span>graph <span class="token operator">=</span> Graph<span class="token punctuation">(</span>NEO4J_URL<span class="token punctuation">,</span> username<span class="token operator">=</span>NEO4J_USERNAME<span class="token punctuation">,</span> password<span class="token operator">=</span>NEO4J_PASSWORD<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div><h3 id="节点"><a href="#节点" class="headerlink" title="节点"></a>节点</h3><h4 id="创建节点"><a href="#创建节点" class="headerlink" title="创建节点"></a>创建节点</h4><p>节点的定义由 <code>Node</code> 实现，要将其添加到 <code>neo4j</code> 数据库中则可通过 <code>create</code> 或 <code>merge</code> 方法实现，两者都可创建节点，但有些许的不同：</p><ul><li><code>create(node)</code> ：即便是属性值相同的同一节点，在调用 <code>create</code> 之后也会创建一个新节点</li><li><code>merge(node, &quot;node_label&quot;, &quot;main_attr&quot;)</code> ：主要是 <code>main_attr</code> （主属性）一样，就视作同一节点，对改节点做的任何更改在 <code>merge</code> 的时候都会覆盖已有的节点 </li></ul><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python">node0 <span class="token operator">=</span> Node<span class="token punctuation">(</span><span class="token string">'Person'</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">'Alice'</span><span class="token punctuation">)</span>  <span class="token comment"># 直接指定属性</span><span class="token comment"># 通过属性dict创建</span>person_info <span class="token operator">=</span> <span class="token punctuation">&#123;</span>    <span class="token string">'name'</span><span class="token punctuation">:</span><span class="token string">'Jhon'</span><span class="token punctuation">&#125;</span>    node1 <span class="token operator">=</span> Node<span class="token punctuation">(</span><span class="token string">'Person'</span><span class="token punctuation">,</span> <span class="token operator">**</span>person_info<span class="token punctuation">)</span>graph<span class="token punctuation">.</span>create<span class="token punctuation">(</span>node0<span class="token punctuation">)</span>   <span class="token comment"># create创建节点，如果不做检查，总是会创建新节点</span>graph<span class="token punctuation">.</span>merge<span class="token punctuation">(</span>node1<span class="token punctuation">,</span><span class="token string">'Person'</span><span class="token punctuation">,</span> <span class="token string">'name'</span><span class="token punctuation">)</span>   <span class="token comment"># 如果节点已存在则覆盖(只要同一key)，需要指定primarykey和primarylabel</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div><h4 id="查找节点"><a href="#查找节点" class="headerlink" title="查找节点"></a>查找节点</h4><ul><li>精确查找（根据指定属性）</li></ul><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python">matcher <span class="token operator">=</span> NodeMatcher<span class="token punctuation">(</span>graph<span class="token punctuation">)</span>matcher<span class="token punctuation">.</span>match<span class="token punctuation">(</span><span class="token string">"Person"</span><span class="token punctuation">,</span> name<span class="token operator">=</span><span class="token string">"Alice"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>first<span class="token punctuation">(</span><span class="token punctuation">)</span>   <span class="token comment"># 返回node</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></div><div class="table-container"><table><thead><tr><th><strong>描述</strong></th><th><strong>后缀</strong></th><th><strong>表达式</strong></th><th><strong>示例</strong></th><th><strong>cypher语句</strong></th></tr></thead><tbody><tr><td>相等</td><td>__exact</td><td>=</td><td>matcher.match(“Person”, name__exact=”Kevin Bacon”)</td><td>MATCH (<em>:Person) WHERE name = “Kevin Bacon” RETURN </em></td></tr><tr><td>不等</td><td>__not</td><td>&lt;&gt;</td><td>matcher.match(“Person”, name__not=”Rick Astley”)</td><td>MATCH (<em>:Person) WHERE </em>.name &lt;&gt; “Rick Astley” RETURN _</td></tr><tr><td>大于</td><td>__gt</td><td>&gt;</td><td>matcher.match(“Person”, born__gt=1985)</td><td>MATCH (<em>:Person) WHERE </em>.born &gt; 1985 RETURN _</td></tr><tr><td>大于等于</td><td>__gte</td><td>&gt;=</td><td>matcher.match(“Person”, born__gte=1965)</td><td>MATCH (<em>:Person) WHERE </em>.born &gt;= 1965 RETURN _</td></tr><tr><td>小于</td><td>__lt</td><td>&lt;</td><td>matcher.match(“Person”, born__lt=1965)</td><td>MATCH (<em>:Person) WHERE </em>.born &lt; 1965 RETURN _</td></tr><tr><td>小于等于</td><td>__lte</td><td>&lt;=</td><td>matcher.match(“Person”, born__lte=1965)</td><td>MATCH (<em>:Person) WHERE </em>.born &lt;= 1965 RETURN _</td></tr><tr><td>以…开头</td><td>__startswith</td><td>STARTS WITH</td><td>matcher.match(“Person”, name__startswith=”Kevin”)</td><td>MATCH (<em>:Person) WHERE </em>.name STARTS WITH “Kevin” RETURN _</td></tr><tr><td>以…结尾</td><td>__endswith</td><td>ENDS WITH</td><td>matcher.match(“Person”, name__endswith=”Smith”)</td><td>MATCH (<em>:Person) WHERE </em>.name ENDS WITH “Smith” RETURN _</td></tr><tr><td>包含</td><td>__contains</td><td>CONTAINS</td><td>matcher.match(“Person”, name__contains=”James”)</td><td>MATCH (<em>:Person) HWERE </em>.name CONTAINS “James” RETURN _</td></tr></tbody></table></div><ul><li>模糊查找（根据正则表达式）</li></ul><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">list(matcher.match(&quot;Person&quot;).where(&quot;_.name &#x3D;~ &#39;K.*&#39;&quot;))  # _指代节点，~代表采用正则表达式<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div><ul><li>查找结果排序与限制</li></ul><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">list(matcher.match(&quot;Person&quot;).where(&quot;_.name &#x3D;~ &#39;K.*&#39;&quot;).order_by(&quot;_.name&quot;).limit(3))<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div><ul><li>查找结果个数</li></ul><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">len(matcher.match(&quot;Person&quot;).where(&quot;_.name &#x3D;~ &#39;K.*&#39;&quot;))<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div><ul><li>跳过前n个查找结果</li></ul><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">list(matcher.match(&quot;Person&quot;).where(&quot;_.name &#x3D;~ &#39;K.*&#39;&quot;).skip(3))<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div><h4 id="节点属性与标签"><a href="#节点属性与标签" class="headerlink" title="节点属性与标签"></a>节点属性与标签</h4><ul><li>属性操作</li></ul><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">node[key] &#x3D; value   # 给节点属性赋值del node[key]   # 删除节点属性len(node)   # 节点属性的个数dict(node)  # 返回字典，包括了该节点的所有属性node.identity   # 返回节点id<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></div><ul><li>标签操作</li></ul><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">node.labels     # 返回节点的所有标签labelA in node.labels   # 如果节点具有标签labelA，返回Truenode.labels.add(labelB)     # 给节点增加标签labelBnode.labels.discard(labelC)     # 删除节点标签labelCnode.labels.remove(labelC)  # 同上，但是如果labelC不存在的话会返回ValueErrornode.labels.clear()     # 清除节点所有标签node.labels.update(manylabels)  # 从可迭代对象manylabels中给节点增加多个标签<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div><h3 id="关系"><a href="#关系" class="headerlink" title="关系"></a>关系</h3><h4 id="创建关系"><a href="#创建关系" class="headerlink" title="创建关系"></a>创建关系</h4><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">properties &#x3D; &#123;    &#39;year&#39; : 1999&#125;ab &#x3D; Relationship(node_a,&#39;knows&#39;, node_b, )  # 后面还可以传入关系的属性字典aa &#x3D; Relationship(node_a,&#39;like&#39;)ab[&#39;time&#39;] &#x3D; &#39;2019&#x2F;01&#x2F;01&#39;  # 给关系添加属性<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div><h4 id="查找关系"><a href="#查找关系" class="headerlink" title="查找关系"></a>查找关系</h4><ul><li>查找所有指定类型的关系</li></ul><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">matcher&#x3D;RelationshipMatcher(graph)list(matcher.match(r_type&#x3D;&#39;KNOWS&#39;))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></div><ul><li>查找指定起始节点或终点的关系</li></ul><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">relation_matcher &#x3D; RelationshipMatcher(graph)result &#x3D; relation_matcher.match((node,), &#39;父亲&#39;).first()  # 注意这里的node需要先按照nodematcher.match得到<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></div><blockquote><p>(node,) 也可以是{node}</p></blockquote><h4 id="关系属性"><a href="#关系属性" class="headerlink" title="关系属性"></a>关系属性</h4><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">relationshipA &#x3D;&#x3D; relationshipBrelationshipA !&#x3D; relationshipB  # 判断两个关系是否相等，但是和节点不同，这里只要起始、终止节点和关系类型相同，就判定两个关系相等del relationship[key]   # 删除属性len(relationship)   # 返回属性个数dict(relationship)  # 返回字典，包括所有属性type(relationship)  # 返回关系的类型relationship.nodes   # 返回关系中的所有节点<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div><h3 id="OGM"><a href="#OGM" class="headerlink" title="OGM"></a>OGM</h3><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">from py2neo.ogm import GraphObject, RelatedTo, RelatedFromclass Movie(GraphObject):    __primarykey__ &#x3D; &quot;title&quot;    title &#x3D; Property()    tag_line &#x3D; Property(&quot;tagline&quot;)    released &#x3D; Property()    actors &#x3D; RelatedFrom(&quot;Person&quot;, &quot;ACTED_IN&quot;)    directors &#x3D; RelatedFrom(&quot;Person&quot;, &quot;DIRECTED&quot;)    producers &#x3D; RelatedFrom(&quot;Person&quot;, &quot;PRODUCED&quot;)class Person(GraphObject):    __primarykey__ &#x3D; &quot;name&quot;    name &#x3D; Property()  # Property(key&#x3D;&#39;姓名&#39;, default&#x3D;&#39;Tom&#39;)    born &#x3D; Property()    acted_in &#x3D; RelatedTo(Movie)  # RelatedTo(Movie,relationship_type&#x3D;&#39;ACTED_IN&#39;), 类名引号可用可不用    directed &#x3D; RelatedTo(Movie)    produced &#x3D; RelatedTo(Movie)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div><h4 id="查找节点-1"><a href="#查找节点-1" class="headerlink" title="查找节点"></a>查找节点</h4><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">Person.match(graph, &quot;Alice&quot;).first()list(Person.match(graph).where(&quot;_.name &#x3D;~ &#39;A.*&#39;&quot;))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></div><h4 id="属性-amp-操作"><a href="#属性-amp-操作" class="headerlink" title="属性&amp;操作"></a>属性&amp;操作</h4><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">alice &#x3D; Person()alice.name &#x3D; &quot;Alice Smith&quot;graph.push(alice)  # 不需要create&#x2F;merge<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></div><h4 id="创建关系-1"><a href="#创建关系-1" class="headerlink" title="创建关系"></a>创建关系</h4><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">cp &#x3D; Person()cp.name &#x3D; &#39;cp&#39;m1 &#x3D; Movie()m1.title &#x3D; &#39;煎饼侠&#39;cp.acted_in.add(m1)  # 添加cp-&gt;acted_in -&gt;m1， add有参数，可用于传入关系的属性，直接传入字典或**dict # cp.acted_in.remove(m1)  # 移除关系graph.push(cp)graph.push(m1)# adddef add(self, obj, properties&#x3D;None, **kwproperties):    &quot;&quot;&quot; Add or update a related object.    :param obj: the :py:class:&#96;.Model&#96; to relate    :param properties: dictionary of properties to attach to the relationship (optional)    :param kwproperties: additional keyword properties (optional)    &quot;&quot;&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div><blockquote><p>RelatedTo是主动的，RelatedFrom是被动的，因此只需要添加RelatedTo关系（cp.acted_in.add(m1)），就可用查询相应m1.actors</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>py2neo</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>scrapy快速上手</title>
    <link href="/article/52452.html"/>
    <url>/article/52452.html</url>
    
    <content type="html"><![CDATA[<h1 id="Scrapy快速上手"><a href="#Scrapy快速上手" class="headerlink" title="Scrapy快速上手"></a>Scrapy快速上手</h1><h2 id="scrapy简介"><a href="#scrapy简介" class="headerlink" title="scrapy简介"></a>scrapy简介</h2><p><code>scrapy</code>是一款爬虫框架，相比于一般的基于<code>requests</code> 自行编写的爬虫，其特点主要包括：</p><ul><li><strong>系统化&amp;结构化</strong>：这意味着你编写的代码能够有很高的扩展性，维护更加容易，对于不擅长设计项目架构的童鞋帮助很大</li><li><strong>高效率</strong>：<code>scrapy</code> 让你在编写程序的同时不需要考虑许多性能上的东西（例如多线程），这些<code>scrapy</code> 都已为你考虑好了，你真正需要关注的是爬虫的解析部分</li><li><strong>一步到位</strong>：这里指的是一个完整的爬虫工作，包括页面下载-&gt;解析-&gt;过滤-&gt;存储几个基本步骤，<code>scrapy</code>都存在相应的模块进行处理，并且这些步骤之间的衔接也由<code>scrapy</code>完成，让你摆脱数据在不同阶段传输的焦虑</li></ul><p><code>scrapy</code>官方文档对于scrapy有着非常鲜明的解释了，包括<strong>一个基础的教程</strong>，<strong>各个模块具体的功能</strong>，以及一些<strong>高级操作</strong>，本文档的目的旨在：<strong>快速上手，略过一些不必要的细节，实现让一个小白也能根据文档快速实现利用<code>scrapy</code>实现一个完整的爬虫。</strong></p><ul><li><a href="https://docs.scrapy.org/en/latest/intro/tutorial.html">官方文档</a></li><li><a href="https://scrapy-docs.readthedocs.io/zh/latest/intro/tutorial.html">中文文档</a></li></ul><h2 id="一个简单的例子"><a href="#一个简单的例子" class="headerlink" title="一个简单的例子"></a>一个简单的例子</h2><blockquote><p>快速上手一个项目的方式就是跑通一个小例子，一方面简洁的程序能更容易理解，另一方面跑通程序能够提高我们对项目的研究兴趣，下面的例子实现的功能是访问请求：<a href="http://quotes.toscrape.com/page/1/，将该页面每个方框内的**文字、作者、标签**信息提取出来，并存储成`json">http://quotes.toscrape.com/page/1/，将该页面每个方框内的**文字、作者、标签**信息提取出来，并存储成`json</a> lines`的形式。</p></blockquote><ul><li>创建一个<code>scrapy</code>项目：<code>scrapy startproject tutorial</code>，你将得到如下结构的项目，目前不必纠结每个文件目录是干什么的</li></ul><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python">tutorial<span class="token operator">/</span>    scrapy<span class="token punctuation">.</span>cfg            <span class="token comment"># deploy configuration file</span>    tutorial<span class="token operator">/</span>             <span class="token comment"># project's Python module, you'll import your code from here</span>        __init__<span class="token punctuation">.</span>py        items<span class="token punctuation">.</span>py          <span class="token comment"># project items definition file</span>        middlewares<span class="token punctuation">.</span>py    <span class="token comment"># project middlewares file</span>        pipelines<span class="token punctuation">.</span>py      <span class="token comment"># project pipelines file</span>        settings<span class="token punctuation">.</span>py       <span class="token comment"># project settings file</span>        spiders<span class="token operator">/</span>          <span class="token comment"># a directory where you'll later put your spiders</span>            __init__<span class="token punctuation">.</span>py<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div><ul><li>在根目录下创建一个<code>spider</code> (针对每一个不同的网站都要创建一个<code>spider</code>) ：<code>scrapy genspider quotes quotes.toscrape.com</code>,这会在<code>spiders</code>下创建一个新的文件，包含以下内容：</li></ul><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> scrapy<span class="token keyword">class</span> <span class="token class-name">QuotesSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>    name <span class="token operator">=</span> <span class="token string">"quotes"</span>    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span>        <span class="token string">'http://quotes.toscrape.com/page/1/'</span><span class="token punctuation">,</span>    <span class="token punctuation">]</span>    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">pass</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div><blockquote><p>这里有一些需要解释的，<code>start_urls</code> 是一个列表，包含了我们需要爬取的网址，同时也是起始网址，可以理解为爬虫入口，很多爬虫会从起始网址开始，一步步提取出更多的网址并进一步爬取，但是在这个例子中，我们只考虑单个网址。<code>parse</code> 是一个解析方法，它有一个参数<code>response</code> ，就是爬取<code>start_urls</code> 列表中的网址后得到的结果，你可以理解为<code>response=requests.get(url=start_requests[i]), i=0,1,2...</code>  ，<strong>注意</strong>，爬虫的核心代码就是从网页中解析出我们需要的数据，因此在你的爬虫文件中编写的其他函数（例如<code>parse_authro(self, response)</code>），应该也遵照这个思想：接收response，解析数据，<code>yield</code> 新请求。有点扯远了，继续下一步</p></blockquote><ul><li>我们需要从<code>response</code>中解析出方框的内容，这来源于我们对于网页源代码的观察，如下图：</li></ul><p><img src="https://gitee.com/gentlecp/ImgUrl/raw/master/20210203181417.png" alt="image-20210203181416969" style="zoom:50%;" /></p><p>容易发现每个方框都是一个<code>class=&quot;quote&quot;</code> 的<code>div</code>标签，因此我们可以用如下的代码去解析其中的数据（代码运用了<code>css</code>解析，后面会详解，这里了解一下就好）</p><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> scrapy<span class="token keyword">class</span> <span class="token class-name">QuotesSpider</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Spider<span class="token punctuation">)</span><span class="token punctuation">:</span>    name <span class="token operator">=</span> <span class="token string">"quotes"</span>    start_urls <span class="token operator">=</span> <span class="token punctuation">[</span>        <span class="token string">'http://quotes.toscrape.com/page/1/'</span><span class="token punctuation">,</span>    <span class="token punctuation">]</span>    <span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> quote <span class="token keyword">in</span> response<span class="token punctuation">.</span>css<span class="token punctuation">(</span><span class="token string">'div.quote'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">yield</span> <span class="token punctuation">&#123;</span>                <span class="token string">'text'</span><span class="token punctuation">:</span> quote<span class="token punctuation">.</span>css<span class="token punctuation">(</span><span class="token string">'span.text::text'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                <span class="token string">'author'</span><span class="token punctuation">:</span> quote<span class="token punctuation">.</span>css<span class="token punctuation">(</span><span class="token string">'small.author::text'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                <span class="token string">'tags'</span><span class="token punctuation">:</span> quote<span class="token punctuation">.</span>css<span class="token punctuation">(</span><span class="token string">'div.tags a.tag::text'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>getall<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div><blockquote><p>上面的代码让我们遍历所有<code>div</code>标签，并从中提取出三个字段的内容，至此，我们的爬虫解析部分就写好了</p></blockquote><ul><li>运行我们的爬虫很容易，直接执行代码:<code>scrapy crawl quotes -o res.jl</code>就可以看到我们的数据了</li></ul><p><img src="https://gitee.com/gentlecp/ImgUrl/raw/master/20210203182053.png" alt="image-20210203182053647" style="zoom:50%;" /></p><h2 id="scrapy运作机制"><a href="#scrapy运作机制" class="headerlink" title="scrapy运作机制"></a>scrapy运作机制</h2><p>pass</p><h2 id="scrapy选择器：css与xpath使用"><a href="#scrapy选择器：css与xpath使用" class="headerlink" title="scrapy选择器：css与xpath使用"></a>scrapy选择器：css与xpath使用</h2><blockquote><p>css&amp;xpath是scrapy中<code>response</code>解析提取数据的两种主要方式，在此之前，你一定用过例如<code>Beautifulsoup</code> 进行网页文本数据的解析，相比之下，<code>scrapy</code>内置的解析器效率要更高，因此强烈建议将这两种解析方法都掌握。下面的列表不会像官网一样一步步教你解析，更多的是一种功能式的查询，即回答<strong>我该怎么样获取我想要的数据</strong>的问题。</p></blockquote><h3 id="查找指定标签文本"><a href="#查找指定标签文本" class="headerlink" title="查找指定标签文本"></a>查找指定标签文本</h3><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">response.css(&#39;a::text&#39;)     response.xpath(&#39;&#x2F;&#x2F;a&#x2F;text()&#39;)   # 有多个标签返回多个<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></div><h3 id="查找指定标签的属性"><a href="#查找指定标签的属性" class="headerlink" title="查找指定标签的属性"></a>查找指定标签的属性</h3><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">response.css(&#39;a::attr(href)&#39;)response.xpath(&#39;&#x2F;&#x2F;a&#x2F;@href&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></div><h3 id="查找指定属性为指定值的标签"><a href="#查找指定属性为指定值的标签" class="headerlink" title="查找指定属性为指定值的标签"></a>查找指定属性为指定值的标签</h3><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">response.css(&#39;a[href&#x3D;&quot;xxx&quot;]&#39;)   # 获取属性href&#x3D;xxx的a标签response.xpath(&#39;&#x2F;&#x2F;a[@href&#x3D;&quot;xxx&quot;]&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></div><h3 id="查找指定属性包含指定值的标签"><a href="#查找指定属性包含指定值的标签" class="headerlink" title="查找指定属性包含指定值的标签"></a>查找指定属性包含指定值的标签</h3><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">response.css(&#39;a[href*&#x3D;&quot;xxx&quot;]&#39;)response.xpath(&#39;a[contains(@href, &quot;xxx&quot;)]&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></div><h3 id="查找文本为指定值的标签"><a href="#查找文本为指定值的标签" class="headerlink" title="查找文本为指定值的标签"></a>查找文本为指定值的标签</h3><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">response.xpath(&#39;&#x2F;&#x2F;a[text()&#x3D;&quot;xxx&quot;]&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div><h3 id="查找文本包含指定值的标签"><a href="#查找文本包含指定值的标签" class="headerlink" title="查找文本包含指定值的标签"></a>查找文本包含指定值的标签</h3><div class="code-wrapper"><pre class="line-numbers language-none"><code class="language-none">response.xpath(&#39;a[contains(text(), &quot;xxx&quot;)]&#39;)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></div><p>当获取到标签之后，可以通过 <code>get()</code> , <code>getall()</code> 等方法提取需要的信息，或者在此基础上再进行标签的获取</p><h2 id="文件-amp-图片下载"><a href="#文件-amp-图片下载" class="headerlink" title="文件&amp;图片下载"></a>文件&amp;图片下载</h2><p>有些时候我们希望爬虫不仅爬取文字信息，也要下载图片&amp;文件内容到本地，这个时候就需要用到<code>scrapy.pipelines.FilesPipeline</code>和<code>scrapy.pipelines.ImagePipeline</code> 了，它们分别定义了文件管道和图片管道。传统的下载方式是在<code>parse</code> 的时候加入下载链接到<code>item</code>中，然后调用下载函数（如<code>cptools.process.download_file</code>）指定下载并存储，但这种下载方式是阻塞式的，用<code>pipeline</code>的好处是可以异步下载，提高效率，主要的步骤如下：</p><ol><li>定义一个<code>Item</code>（可以是原本已有的），添加两个字段<code>files</code>,<code>file_urls</code>，后者是下载链接的列表（即一次可传入多个下载链接），前者是文件下载完成后存储的下载相关信息（如下载路径、url、校验码等）</li><li>在<code>settings.py</code> 中设置变量<code>FILES_STORE</code>即存储路径，之后下载的内容都会存储在这个路径下，最终结果是<code>FILES_STORE/full/3afec3b4765f8f0a07b78f98c07b83f013567a0a</code>(最后一串是文件的sha1值命名的文件)</li><li>在<code>settings.py</code>中启动<code>pipeline</code>：即<code>scrapy.pipelines.files.FilesPipeline:1</code> </li></ol><p>下图是一个例子：</p><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># items.py</span><span class="token keyword">class</span> <span class="token class-name">SFItem</span><span class="token punctuation">(</span>scrapy<span class="token punctuation">.</span>Item<span class="token punctuation">)</span><span class="token punctuation">:</span>    name <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 固件名称</span>    <span class="token comment"># 下载信息存储，下面字段必要</span>    file_urls <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span>    files <span class="token operator">=</span> scrapy<span class="token punctuation">.</span>Field<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># spiders</span><span class="token keyword">def</span> <span class="token function">parse</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>item <span class="token operator">=</span> SFItem<span class="token punctuation">(</span><span class="token punctuation">)</span>    item<span class="token punctuation">[</span><span class="token string">'file_urls'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"https://download_url.zip"</span><span class="token punctuation">]</span>   <span class="token comment"># 注意要列表传入</span>    <span class="token keyword">yield</span> item    <span class="token comment"># settings.py</span>ITEM_PIPELINES <span class="token operator">=</span> <span class="token punctuation">&#123;</span>    <span class="token string">'IndustryInfoCrawler.pipelines.SFFilesPipeline'</span><span class="token punctuation">:</span> <span class="token number">1</span><span class="token punctuation">,</span>   <span class="token string">'IndustryInfoCrawler.pipelines.MongoPipeline'</span><span class="token punctuation">:</span> <span class="token number">300</span><span class="token punctuation">,</span><span class="token punctuation">&#125;</span>FILES_STORE <span class="token operator">=</span> <span class="token string">'root'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div><blockquote><p>上面的方式基本满足了对文件的下载操作，但有两点问题：</p><ol><li><p>下载路径我们只定义了根目录，如果我们相对文件分类，那怎么办</p></li><li><p>文件名sha1值是为了不重复，但可读性很差，我们希望能够自定义文件名</p></li></ol></blockquote><p>解决办法通过重写<code>Pipeline</code>实现，样例如下：</p><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> scrapy<span class="token punctuation">.</span>pipelines<span class="token punctuation">.</span>images <span class="token keyword">import</span> FilesPipeline<span class="token keyword">class</span> <span class="token class-name">SFFilesPipeline</span><span class="token punctuation">(</span>FilesPipeline<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    自定义下载管道    """</span>    <span class="token keyword">def</span> <span class="token function">get_media_requests</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> item<span class="token punctuation">,</span> info<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> file_url <span class="token keyword">in</span> item<span class="token punctuation">[</span><span class="token string">'file_urls'</span><span class="token punctuation">]</span><span class="token punctuation">:</span>            <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>image_url<span class="token punctuation">)</span>  <span class="token comment"># 请求下载</span>    <span class="token keyword">def</span> <span class="token function">file_path</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> request<span class="token punctuation">,</span> response<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> info<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">,</span> item<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>         <span class="token comment"># parent = super().file_path(request, response, info)  # 获取父类目录</span>        SF_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>FILES_STORE<span class="token punctuation">,</span> <span class="token string">'SF'</span><span class="token punctuation">)</span>  <span class="token comment"># 在这里自定义存储的目录，也可以根据不同文件类型，自己设置分类目录</span>        <span class="token keyword">if</span> <span class="token keyword">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>exists<span class="token punctuation">(</span>SF_path<span class="token punctuation">)</span><span class="token punctuation">:</span>            os<span class="token punctuation">.</span>mkdir<span class="token punctuation">(</span>SF_path<span class="token punctuation">)</span>        filename <span class="token operator">=</span> <span class="token string">'test.zip'</span>        <span class="token keyword">return</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>SF_path<span class="token punctuation">,</span> filename<span class="token punctuation">)</span>  <span class="token comment"># 返回结果为文件存储路径</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div><ul><li>文件到期：很多时候我们不希望重复对本地已有的文件进行下载操作，可以自定义下载管道中请求的时候检查本地文件是否存在，也可以通过设置文件到期时间（未到期的文件不会重复下载）</li></ul><div class="code-wrapper"><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># settings.py</span><span class="token comment"># 120 days of delay for files expiration</span>FILES_EXPIRES <span class="token operator">=</span> <span class="token number">120</span><span class="token comment"># 30 days of delay for images expiration</span>IMAGES_EXPIRES <span class="token operator">=</span> <span class="token number">30</span><span class="token comment"># 如果存在自定义的文件管道，如上面的SFFilesPipeline，可以对单个子类管道设置到期时间，以子类名称大写开头</span>SFFILESPIPELINE_FILES_EXPIRES <span class="token operator">=</span> <span class="token number">120</span>   <span class="token comment"># 120天内不重复下载通过SFFilesPipeline的文件</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></div><blockquote><p>还有一个注意点是，当下载文件过大，超出scrapy预期的话，会报警告，可以通过在<code>settings.py</code>中设置<code>DOWNLOAD_WARNSIZE = 0</code>去除</p></blockquote><ul><li><a href="https://blog.csdn.net/Zhihua_W/article/details/105200916">https://blog.csdn.net/Zhihua_W/article/details/105200916</a></li><li><a href="https://blog.csdn.net/weixin_43343144/article/details/87908448">https://blog.csdn.net/weixin_43343144/article/details/87908448</a></li><li><a href="https://scrapy-docs.readthedocs.io/zh/latest/topics/media-pipeline.html?highlight=%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6">https://scrapy-docs.readthedocs.io/zh/latest/topics/media-pipeline.html?highlight=%E4%B8%8B%E8%BD%BD%E6%96%87%E4%BB%B6</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>python</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>scrapy</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
